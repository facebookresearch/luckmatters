# embedding dimension
emsize: 200 
# dimension of the feedforward network model in ``nn.TransformerEncoder``
d_hid: 200  
# number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``
nlayers: 1  
# number of heads in ``nn.MultiheadAttention``
nhead: 1  
# dropout = 0.1  # dropout probability
dropout: 0.2

batch_size: 20 
eval_batch_size: 10

bptt: 35

seed: 1

num_epoch: 5
lr_y_multi_on_z: 1
lr_z: 1

log_interval: 100
save_interval: 1000

dataset: wikitext2

eval_only: false
eval_last: false

eval_models: null

use_baseline: false

yzformer:
  num_layers: 3
  attn_include_base_token: false
  zero_init: true
  normalize: true
  residual: false  
  seq_first: true
  bottleneck_dim: 256
