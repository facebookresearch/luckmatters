{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "from typing import List\n",
    "from typing import Optional, Tuple\n",
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import dataset\n",
    "from transformers import PreTrainedModel\n",
    "from torch.nn.modules.linear import NonDynamicallyQuantizableLinear\n",
    "from torch.nn.init import constant_, xavier_normal_, xavier_uniform_\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from transformers import PretrainedConfig\n",
    "from transformers import OpenAIGPTConfig, AutoTokenizer, OpenAIGPTLMHeadModel \n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copied from torch transformer implementation, because we want to customize some outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information\n",
    "    from different representation subspaces as described in the paper:\n",
    "    `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\n",
    "    Multi-Head Attention is defined as:\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
    "    ``forward()`` will use a special optimized implementation if all of the following\n",
    "    conditions are met:\n",
    "    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor. This\n",
    "      restriction will be loosened in the future.)\n",
    "    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n",
    "    - training is disabled (using ``.eval()``)\n",
    "    - dropout is 0\n",
    "    - ``add_bias_kv`` is ``False``\n",
    "    - ``add_zero_attn`` is ``False``\n",
    "    - ``batch_first`` is ``True`` and the input is batched\n",
    "    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n",
    "    - at most one of ``key_padding_mask`` or ``attn_mask`` is passed\n",
    "    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n",
    "      nor ``attn_mask`` is passed\n",
    "    If the optimized implementation is in use, a\n",
    "    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n",
    "    ``query``/``key``/``value`` to represent padding more efficiently than using a\n",
    "    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n",
    "    will be returned, and an additional speedup proportional to the fraction of the input\n",
    "    that is padding can be expected.\n",
    "    Args:\n",
    "        embed_dim: Total dimension of the model.\n",
    "        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
    "            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
    "        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n",
    "        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n",
    "        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n",
    "        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n",
    "            Default: ``False``.\n",
    "        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n",
    "        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "    Examples::\n",
    "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n",
    "                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        if self._qkv_same_embed_dim is False:\n",
    "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n",
    "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n",
    "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "        else:\n",
    "            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
    "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "        \n",
    "        \n",
    "#         self.linear1 = Linear(embed_dim, embed_dim, **factory_kwargs)\n",
    "#         self.linear2 = Linear(embed_dim, embed_dim, **factory_kwargs)\n",
    "#         self.linear3 = Linear(embed_dim, embed_dim, **factory_kwargs)\n",
    "#         self.activation = F.relu\n",
    "#         self.dropout2 = Dropout(0.3)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super(MultiheadAttention, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\n",
    "                need_weights: bool = True, attn_mask: Optional[Tensor] = None,\n",
    "                average_attn_weights: bool = True) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        r\"\"\"\n",
    "    Args:\n",
    "        query: Query embeddings of shape :math:`(L, E_q)` for unbatched input, :math:`(L, N, E_q)` when ``batch_first=False``\n",
    "            or :math:`(N, L, E_q)` when ``batch_first=True``, where :math:`L` is the target sequence length,\n",
    "            :math:`N` is the batch size, and :math:`E_q` is the query embedding dimension ``embed_dim``.\n",
    "            Queries are compared against key-value pairs to produce the output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key: Key embeddings of shape :math:`(S, E_k)` for unbatched input, :math:`(S, N, E_k)` when ``batch_first=False``\n",
    "            or :math:`(N, S, E_k)` when ``batch_first=True``, where :math:`S` is the source sequence length,\n",
    "            :math:`N` is the batch size, and :math:`E_k` is the key embedding dimension ``kdim``.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        value: Value embeddings of shape :math:`(S, E_v)` for unbatched input, :math:`(S, N, E_v)` when\n",
    "            ``batch_first=False`` or :math:`(N, S, E_v)` when ``batch_first=True``, where :math:`S` is the source\n",
    "            sequence length, :math:`N` is the batch size, and :math:`E_v` is the value embedding dimension ``vdim``.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``\n",
    "            to ignore for the purpose of attention (i.e. treat as \"padding\"). For unbatched `query`, shape should be :math:`(S)`.\n",
    "            Binary and byte masks are supported.\n",
    "            For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for\n",
    "            the purpose of attention. For a byte mask, a non-zero value indicates that the corresponding ``key``\n",
    "            value will be ignored.\n",
    "        need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.\n",
    "            Default: ``True``.\n",
    "        attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\n",
    "            :math:`(L, S)` or :math:`(N\\cdot\\text{num\\_heads}, L, S)`, where :math:`N` is the batch size,\n",
    "            :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be\n",
    "            broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\n",
    "            Binary, byte, and float masks are supported. For a binary mask, a ``True`` value indicates that the\n",
    "            corresponding position is not allowed to attend. For a byte mask, a non-zero value indicates that the\n",
    "            corresponding position is not allowed to attend. For a float mask, the mask values will be added to\n",
    "            the attention weight.\n",
    "        average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n",
    "            heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n",
    "            effect when ``need_weights=True``. Default: ``True`` (i.e. average weights across heads)\n",
    "    Outputs:\n",
    "        - **attn_output** - Attention outputs of shape :math:`(L, E)` when input is unbatched,\n",
    "          :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` when ``batch_first=True``,\n",
    "          where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` is the\n",
    "          embedding dimension ``embed_dim``.\n",
    "        - **attn_output_weights** - Only returned when ``need_weights=True``. If ``average_attn_weights=True``,\n",
    "          returns attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n",
    "          :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n",
    "          :math:`S` is the source sequence length. If ``average_weights=False``, returns attention weights per\n",
    "          head of shape :math:`(\\text{num\\_heads}, L, S)` when input is unbatched or :math:`(N, \\text{num\\_heads}, L, S)`.\n",
    "        .. note::\n",
    "            `batch_first` argument is ignored for unbatched inputs.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        is_batched = query.dim() == 3\n",
    "        why_not_fast_path = ''\n",
    "        if not is_batched:\n",
    "            why_not_fast_path = f\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\n",
    "        elif query is not key or key is not value:\n",
    "            # When lifting this restriction, don't forget to either\n",
    "            # enforce that the dtypes all match or test cases where\n",
    "            # they don't!\n",
    "            why_not_fast_path = \"non-self attention was used (query, key, and value are not the same Tensor)\"\n",
    "        elif self.in_proj_bias is not None and query.dtype != self.in_proj_bias.dtype:\n",
    "            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\n",
    "        elif self.in_proj_weight is not None and query.dtype != self.in_proj_weight.dtype:\n",
    "            # this case will fail anyway, but at least they'll get a useful error message.\n",
    "            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n",
    "        elif self.training:\n",
    "            why_not_fast_path = \"training is enabled\"\n",
    "        elif not self.batch_first:\n",
    "            why_not_fast_path = \"batch_first was not True\"\n",
    "        elif self.bias_k is not None:\n",
    "            why_not_fast_path = \"self.bias_k was not None\"\n",
    "        elif self.bias_v is not None:\n",
    "            why_not_fast_path = \"self.bias_v was not None\"\n",
    "        elif self.dropout:\n",
    "            why_not_fast_path = f\"dropout was {self.dropout}, required zero\"\n",
    "        elif self.add_zero_attn:\n",
    "            why_not_fast_path = \"add_zero_attn was enabled\"\n",
    "        elif not self._qkv_same_embed_dim:\n",
    "            why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n",
    "        elif attn_mask is not None:\n",
    "            why_not_fast_path = \"attn_mask was not None\"\n",
    "        elif query.is_nested and key_padding_mask is not None:\n",
    "            why_not_fast_path = \"key_padding_mask is not supported with NestedTensor input\"\n",
    "\n",
    "        if not why_not_fast_path:\n",
    "            tensor_args = (\n",
    "                query,\n",
    "                key,\n",
    "                value,\n",
    "                self.in_proj_weight,\n",
    "                self.in_proj_bias,\n",
    "                self.out_proj.weight,\n",
    "                self.out_proj.bias,\n",
    "            )\n",
    "            # We have to use list comprehensions below because TorchScript does not support\n",
    "            # generator expressions.\n",
    "            if torch.overrides.has_torch_function(tensor_args):\n",
    "                why_not_fast_path = \"some Tensor argument has_torch_function\"\n",
    "            elif not all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):\n",
    "                why_not_fast_path = \"some Tensor argument is neither CUDA nor CPU\"\n",
    "            elif torch.is_grad_enabled() and any([x.requires_grad for x in tensor_args]):\n",
    "                why_not_fast_path = (\"grad is enabled and at least one of query or the \"\n",
    "                                     \"input/output projection weights or biases requires_grad\")\n",
    "            if not why_not_fast_path:\n",
    "                return torch._native_multi_head_attention(\n",
    "                    query,\n",
    "                    key,\n",
    "                    value,\n",
    "                    self.embed_dim,\n",
    "                    self.num_heads,\n",
    "                    self.in_proj_weight,\n",
    "                    self.in_proj_bias,\n",
    "                    self.out_proj.weight,\n",
    "                    self.out_proj.bias,\n",
    "                    key_padding_mask if key_padding_mask is not None else attn_mask,\n",
    "                    need_weights,\n",
    "                    average_attn_weights)\n",
    "        any_nested = query.is_nested or key.is_nested or value.is_nested\n",
    "        assert not any_nested, (\"MultiheadAttention does not support NestedTensor outside of its fast path. \" +\n",
    "                                f\"The fast path was not hit because {why_not_fast_path}\")\n",
    "\n",
    "        if self.batch_first and is_batched:\n",
    "            # make sure that the transpose op does not affect the \"is\" property\n",
    "            if key is value:\n",
    "                if query is key:\n",
    "                    query = key = value = query.transpose(1, 0)\n",
    "                else:\n",
    "                    query, key = [x.transpose(1, 0) for x in (query, key)]\n",
    "                    value = key\n",
    "            else:\n",
    "                query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n",
    "\n",
    "#         query = self.dropout2(self.activation(self.linear1(query)))  \n",
    "#         key = self.dropout2(self.activation(self.linear2(key)))  \n",
    "#         value = self.dropout2(self.activation(self.linear3(value)))  \n",
    "        \n",
    "        if not self._qkv_same_embed_dim:\n",
    "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight, average_attn_weights=average_attn_weights)\n",
    "        else:\n",
    "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, average_attn_weights=average_attn_weights)\n",
    "        \n",
    "        if self.batch_first and is_batched:\n",
    "            return attn_output.transpose(1, 0), attn_output_weights\n",
    "        else:\n",
    "            return attn_output, attn_output_weights\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    # FIXME: copy.deepcopy() is not defined on nn.module\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class TransformerEncoder(Module):\n",
    "    r\"\"\"TransformerEncoder is a stack of N encoder layers\n",
    "    Args:\n",
    "        encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
    "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "        enable_nested_tensor: if True, input will automatically convert to nested tensor\n",
    "            (and convert back on output). This will improve the overall performance of\n",
    "            TransformerEncoder when padding rate is high. Default: ``False`` (disabled).\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = transformer_encoder(src)\n",
    "    \"\"\"\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None, enable_nested_tensor=False):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.enable_nested_tensor = enable_nested_tensor\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layers in turn.\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = src\n",
    "        convert_to_nested = False\n",
    "        first_layer = self.layers[0]\n",
    "        if isinstance(first_layer, torch.nn.TransformerEncoderLayer):\n",
    "            if (not first_layer.norm_first and not first_layer.training and\n",
    "                    first_layer.self_attn.batch_first and\n",
    "                    first_layer.self_attn._qkv_same_embed_dim and first_layer.activation_relu_or_gelu and\n",
    "                    first_layer.norm1.eps == first_layer.norm2.eps and\n",
    "                    src.dim() == 3 and self.enable_nested_tensor) :\n",
    "                if src_key_padding_mask is not None and not output.is_nested and mask is None:\n",
    "                    tensor_args = (\n",
    "                        src,\n",
    "                        first_layer.self_attn.in_proj_weight,\n",
    "                        first_layer.self_attn.in_proj_bias,\n",
    "                        first_layer.self_attn.out_proj.weight,\n",
    "                        first_layer.self_attn.out_proj.bias,\n",
    "                        first_layer.norm1.weight,\n",
    "                        first_layer.norm1.bias,\n",
    "                        first_layer.norm2.weight,\n",
    "                        first_layer.norm2.bias,\n",
    "                        first_layer.linear1.weight,\n",
    "                        first_layer.linear1.bias,\n",
    "                        first_layer.linear2.weight,\n",
    "                        first_layer.linear2.bias,\n",
    "                    )\n",
    "                    if not torch.overrides.has_torch_function(tensor_args):\n",
    "                        if not torch.is_grad_enabled() or all([not x.requires_grad for x in tensor_args]):\n",
    "                            if output.is_cuda or 'cpu' in str(output.device):\n",
    "                                convert_to_nested = True\n",
    "                                output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())\n",
    "        attentions = []\n",
    "        for mod in self.layers:\n",
    "            if convert_to_nested:\n",
    "                output = mod(output, src_mask=mask)\n",
    "            else:\n",
    "                output, attention = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "                attentions.append(attention)\n",
    "        if convert_to_nested:\n",
    "            output = output.to_padded_tensor(0.)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output, attentions\n",
    "\n",
    "\n",
    "class TransformerDecoder(Module):\n",
    "    r\"\"\"TransformerDecoder is a stack of N decoder layers\n",
    "    Args:\n",
    "        decoder_layer: an instance of the TransformerDecoderLayer() class (required).\n",
    "        num_layers: the number of sub-decoder-layers in the decoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "    Examples::\n",
    "        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        >>> transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "        >>> memory = torch.rand(10, 32, 512)\n",
    "        >>> tgt = torch.rand(20, 32, 512)\n",
    "        >>> out = transformer_decoder(tgt, memory)\n",
    "    \"\"\"\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = tgt\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory, tgt_mask=tgt_mask,\n",
    "                         memory_mask=memory_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class TransformerEncoderLayer(Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of the intermediate layer, can be a string\n",
    "            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n",
    "            operations, respectivaly. Otherwise it's done after. Default: ``False`` (after).\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    Alternatively, when ``batch_first`` is ``True``:\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "        >>> src = torch.rand(32, 10, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    Fast path:\n",
    "        forward() will use a special optimized implementation if all of the following\n",
    "        conditions are met:\n",
    "        - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor\n",
    "          argument ``requires_grad``\n",
    "        - training is disabled (using ``.eval()``)\n",
    "        - batch_first is ``True`` and the input is batched (i.e., ``src.dim() == 3``)\n",
    "        - norm_first is ``False`` (this restriction may be loosened in the future)\n",
    "        - activation is one of: ``\"relu\"``, ``\"gelu\"``, ``torch.functional.relu``, or ``torch.functional.gelu``\n",
    "        - at most one of ``src_mask`` and ``src_key_padding_mask`` is passed\n",
    "        - if src is a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_, neither ``src_mask``\n",
    "          nor ``src_key_padding_mask`` is passed\n",
    "        - the two ``LayerNorm`` instances have a consistent ``eps`` value (this will naturally be the case\n",
    "          unless the caller has manually modified one without modifying the other)\n",
    "        If the optimized implementation is in use, a\n",
    "        `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be\n",
    "        passed for ``src`` to represent padding more efficiently than using a padding\n",
    "        mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ will be\n",
    "        returned, and an additional speedup proportional to the fraction of the input that\n",
    "        is padding can be expected.\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first', 'norm_first']\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        # Legacy string support for activation function.\n",
    "        if isinstance(activation, str):\n",
    "            activation = _get_activation_fn(activation)\n",
    "\n",
    "        # We can't test self.activation in forward() in TorchScript,\n",
    "        # so stash some information about it instead.\n",
    "        if activation is F.relu:\n",
    "            self.activation_relu_or_gelu = 1\n",
    "        elif activation is F.gelu:\n",
    "            self.activation_relu_or_gelu = 2\n",
    "        else:\n",
    "            self.activation_relu_or_gelu = 0\n",
    "        self.activation = activation\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(TransformerEncoderLayer, self).__setstate__(state)\n",
    "        if not hasattr(self, 'activation'):\n",
    "            self.activation = F.relu\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "\n",
    "        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n",
    "\n",
    "        if (src.dim() == 3 and not self.norm_first and not self.training and\n",
    "            self.self_attn.batch_first and\n",
    "            self.self_attn._qkv_same_embed_dim and self.activation_relu_or_gelu and\n",
    "            self.norm1.eps == self.norm2.eps and\n",
    "            src_mask is None and\n",
    "                not (src.is_nested and src_key_padding_mask is not None)):\n",
    "            tensor_args = (\n",
    "                src,\n",
    "                self.self_attn.in_proj_weight,\n",
    "                self.self_attn.in_proj_bias,\n",
    "                self.self_attn.out_proj.weight,\n",
    "                self.self_attn.out_proj.bias,\n",
    "                self.norm1.weight,\n",
    "                self.norm1.bias,\n",
    "                self.norm2.weight,\n",
    "                self.norm2.bias,\n",
    "                self.linear1.weight,\n",
    "                self.linear1.bias,\n",
    "                self.linear2.weight,\n",
    "                self.linear2.bias,\n",
    "            )\n",
    "            if (not torch.overrides.has_torch_function(tensor_args) and\n",
    "                    # We have to use a list comprehension here because TorchScript\n",
    "                    # doesn't support generator expressions.\n",
    "                    all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]) and\n",
    "                    (not torch.is_grad_enabled() or all([not x.requires_grad for x in tensor_args]))):\n",
    "                return torch._transformer_encoder_layer_fwd(\n",
    "                    src,\n",
    "                    self.self_attn.embed_dim,\n",
    "                    self.self_attn.num_heads,\n",
    "                    self.self_attn.in_proj_weight,\n",
    "                    self.self_attn.in_proj_bias,\n",
    "                    self.self_attn.out_proj.weight,\n",
    "                    self.self_attn.out_proj.bias,\n",
    "                    self.activation_relu_or_gelu == 2,\n",
    "                    False,  # norm_first, currently not supported\n",
    "                    self.norm1.eps,\n",
    "                    self.norm1.weight,\n",
    "                    self.norm1.bias,\n",
    "                    self.norm2.weight,\n",
    "                    self.norm2.bias,\n",
    "                    self.linear1.weight,\n",
    "                    self.linear1.bias,\n",
    "                    self.linear2.weight,\n",
    "                    self.linear2.bias,\n",
    "                    src_mask if src_mask is not None else src_key_padding_mask,  # TODO: split into two args\n",
    "                )\n",
    "        x = src\n",
    "        if self.norm_first:\n",
    "            output, attention = self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)\n",
    "            x = x + output\n",
    "            x = x + self._ff_block(self.norm2(x))\n",
    "        else:\n",
    "            output, attention = self._sa_block(x, src_mask, src_key_padding_mask)\n",
    "            x = self.norm1(x + output)\n",
    "            x = self.norm2(x + self._ff_block(x))\n",
    "\n",
    "        return x, attention\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x: Tensor,\n",
    "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x, attn = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=True)\n",
    "        return self.dropout1(x), attn\n",
    "\n",
    "    # feed forward block\n",
    "    def _ff_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransConfig(PretrainedConfig):\n",
    "    model_type = \"transformer\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.ntoken = ntoken\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.d_hid = d_hid\n",
    "        self.nlayers = nlayers\n",
    "        self.dropout=dropout\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class TransformerModel(PreTrainedModel):\n",
    "    \n",
    "    config_class = TransConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model_type = config.model_type\n",
    "        self.pos_encoder = PositionalEncoding(config.d_model, config.dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(config.d_model, config.nhead, config.d_hid, config.dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, config.nlayers)\n",
    "        self.encoder = nn.Embedding(config.ntoken, config.d_model)\n",
    "        self.d_model = config.d_model\n",
    "        self.decoder = nn.Linear(config.d_model, config.ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output, attention = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output, attention\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copied from transformer tutorial \n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n",
    "bptt = 35\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "    \n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 1  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 1  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.3  # dropout probability\n",
    "config = TransConfig(ntokens, emsize, nhead, d_hid, nlayers, dropout)\n",
    "model = TransformerModel(config).to(device)\n",
    "\n",
    "my_list = ['transformer_encoder.layers.0.linear1.weight',\n",
    "'transformer_encoder.layers.0.linear1.bias',\n",
    "'transformer_encoder.layers.0.linear2.weight',\n",
    "'transformer_encoder.layers.0.linear2.bias']\n",
    "params = list(filter(lambda kv: kv[0] in my_list, model.named_parameters()))\n",
    "base_params = list(filter(lambda kv: kv[0] not in my_list, model.named_parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1  # learning rate\n",
    "optimizer = torch.optim.SGD([{'params': [temp[1] for temp in params], 'lr': 10}, {'params': [temp[1] for temp in base_params], 'lr': 5}])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module, epoch=0) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 100\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        \n",
    "        if (batch+(epoch-1)*num_batches) % log_interval == 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "            best_model_params_path = os.path.join('hello', f'model_medium_iter_{batch+(epoch-1)*num_batches}.pt')\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        seq_len = data.size(0)\n",
    "        if seq_len != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:seq_len, :seq_len]\n",
    "        output, attention = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    return_attn = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output, attention = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "            if i==100*bptt:\n",
    "                return_attn = attention\n",
    "                print(data[:,0])\n",
    "                break\n",
    "    return total_loss / (len(eval_data) - 1), return_attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are evaluating the model we just saved during the training. And we're going to save some tensors we plan to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.37 | test ppl     1.44\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.25 | test ppl     1.29\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.25 | test ppl     1.28\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.24 | test ppl     1.27\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.24 | test ppl     1.27\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.23 | test ppl     1.26\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.23 | test ppl     1.26\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.23 | test ppl     1.26\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.22 | test ppl     1.25\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.22 | test ppl     1.25\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.22 | test ppl     1.25\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.22 | test ppl     1.24\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.22 | test ppl     1.24\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.22 | test ppl     1.25\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.22 | test ppl     1.24\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.22 | test ppl     1.24\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.22 | test ppl     1.24\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.22 | test ppl     1.25\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.22 | test ppl     1.24\n",
      "=========================================================================================\n",
      "tensor([   43,  6971,     3,    66,   407,     1,  3476,     2,     1,   319,\n",
      "          157,    29,   842,    19, 13722,     7,   294,    24,    14,  5902,\n",
      "           14,   687,     2,    18,     1,  5832,    16,     1,   157,   162,\n",
      "         1963,   735,    60,  1946,    89], device='cuda:0')\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.22 | test ppl     1.24\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "collections = {}\n",
    "\n",
    "for i in range(0, 2000, 100):\n",
    "    best_model_params_path = os.path.join('hello', f'model_medium_iter_{i}.pt')\n",
    "    model.load_state_dict(torch.load(best_model_params_path))\n",
    "    test_loss, attn = evaluate(model, train_data)\n",
    "    test_ppl = math.exp(test_loss)\n",
    "    print('=' * 89)\n",
    "    print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "          f'test ppl {test_ppl:8.2f}')\n",
    "    print('=' * 89)\n",
    "    collections['iter_'+str(i)] = attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,4,figsize=(16,4))\n",
    "layer = 0\n",
    "\n",
    "for j in range(4):\n",
    "    iteration = 0+500*(j)\n",
    "    attn_map = collections['iter_'+str(iteration)][layer]\n",
    "    axs[j].set_title(\"iter-\"+str(iteration))\n",
    "    axs[j].imshow(attn_map[0].cpu())\n",
    "plt.savefig(\"layer\"+ str(layer) +\".pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
